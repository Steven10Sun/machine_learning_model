{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad8e022c-95a2-430e-b2db-ca237285b0c2",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797f92e4-504c-4fb7-95ce-5391aa1063bb",
   "metadata": {},
   "source": [
    "### Model\n",
    "#### $P(y|X)=\\frac{P(X|y)P(y)}{P(X)}$\n",
    "\n",
    "if you have a lot of variables: <br>\n",
    "#### $P(y|x_1, ..., x_n)=\\frac{P(x_1|y)P(x_2|y)...P(x_n|y)P(y)}{P(x_1)P(x_2)...P(x_n)}$\n",
    "\n",
    "__*!!! since the denominator is the same for both probabilities, it can be omitted from the calculation, and only need to consider the numerator.*__\n",
    "###### $P(y|x_1, ..., x_n)=P(x_1|y)P(x_2|y)...P(x_n|y)P(y)$ \n",
    "<br>\n",
    "Naive Bayes is to use variable X to classify target y based on comparasion of probability of being target 1, target 2, target n\n",
    "\n",
    "**Step**\n",
    "1. separate the dataset by target\n",
    "2. calculate the probability of each target: $P(Y)$\n",
    "3. for loop each target group:\n",
    "    * sum up the frequency for each unique word\n",
    "    * calculate the probability of each word in the target group, **remark: it is $P(X|y)$, conditional prob of x given by target y**\n",
    "    \n",
    "Now, we have $P(Y)$, $P(X|y)$ and P(X)<br>\n",
    "**Input new data** <br>\n",
    "for loop each target group:\n",
    "1. multiple the conditional probability for the input data: $P(y|x_1, ..., x_n)=\\frac{P(x_1|y)P(x_2|y)...P(x_n|y)P(y)}{P(x_1)P(x_2)...P(x_n)}$\n",
    "2. compare probability (likelihood) and assign the target with highest prob to the input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84fb9739-db08-4e9f-a8e2-97b9d9d4126c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a6400f24-e3fb-4ea2-a5de-57ae34c734f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a865d50-e918-43d4-a3a3-885f0b6b2e88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ham_path = 'data/email/ham'\n",
    "spam_path = 'data/email/spam/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a422b810-efbf-44a6-88ba-539c48ae33fb",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/email/ham\\1.txt\n",
      "Hi Peter,\n",
      "\n",
      "With Jose out of town, do you want to\n",
      "meet once in a while to keep things\n",
      "going and do some interesting stuff?\n",
      "\n",
      "Let me know\n",
      "Eugene\n",
      "data/email/ham\\10.txt\n",
      "Ryan Whybrew commented on your status.\n",
      "\n",
      "Ryan wrote:\n",
      "\"turd ferguson or butt horn.\"\n",
      "\n",
      "data/email/ham\\11.txt\n",
      "Arvind Thirumalai commented on your status.\n",
      "\n",
      "Arvind wrote:\n",
      "\"\"you know\"\"\n",
      "\n",
      "\n",
      "Reply to this email to comment on this status.\n",
      "\n",
      "\n",
      "data/email/ham\\12.txt\n",
      "Thanks Peter.\n",
      "\n",
      "I'll definitely check in on this. How is your book\n",
      "going? I heard chapter 1 came in and it was in \n",
      "good shape. ;-)\n",
      "\n",
      "I hope you are doing well.\n",
      "\n",
      "Cheers,\n",
      "\n",
      "Troy\n",
      "data/email/ham\\13.txt\n",
      "Jay Stepp commented on your status.\n",
      "\n",
      "Jay wrote:\n",
      "\"\"to the\" ???\"\n",
      "\n",
      "\n",
      "Reply to this email to comment on this status.\n",
      "\n",
      "To see the comment thread, follow the link below:\n",
      "\n",
      "\n",
      "data/email/ham\\14.txt\n",
      "LinkedIn\n",
      "\n",
      "Kerry Haloney requested to add you as a connection on LinkedIn:\n",
      "\n",
      "Peter,\n",
      "\n",
      "I'd like to add you to my professional network on LinkedIn.\n",
      "\n",
      "- Kerry Haloney\n",
      " \n",
      "\n",
      "data/email/ham\\15.txt\n",
      "Hi Peter,\n",
      " \n",
      "The hotels are the ones that rent out the tent. They are all lined up on the hotel grounds : )) So much for being one with nature, more like being one with a couple dozen tour groups and nature.\n",
      "I have about 100M of pictures from that trip. I can go through them and get you jpgs of my favorite scenic pictures.\n",
      " \n",
      "Where are you and Jocelyn now? New York? Will you come to Tokyo for Chinese New Year? Perhaps to see the two of you then. I will go to Thailand for winter holiday to see my mom : )\n",
      " \n",
      "Take care,\n",
      "D\n",
      "\n",
      "data/email/ham\\16.txt\n",
      "yeah I am ready.  I may not be here because Jar Jar has plane tickets to Germany for me.  \n",
      "data/email/ham\\17.txt\n",
      "Benoit Mandelbrot 1924-2010\n",
      "\n",
      "Benoit Mandelbrot 1924-2010\n",
      "\n",
      "Wilmott Team\n",
      "\n",
      "Benoit Mandelbrot, the mathematician, the father of fractal mathematics, and advocate of more sophisticated modelling in quantitative finance, died on 14th October 2010 aged 85.\n",
      "\n",
      "Wilmott magazine has often featured Mandelbrot, his ideas, and the work of others inspired by his fundamental insights.\n",
      "\n",
      "You must be logged on to view these articles from past issues of Wilmott Magazine.\n",
      "data/email/ham\\18.txt\n",
      "Hi Peter,\n",
      "\n",
      "    Sure thing.  Sounds good.  Let me know what time would be good for you.\n",
      "I will come prepared with some ideas and we can go from there.\n",
      "\n",
      "Regards,\n",
      "\n",
      "-Vivek.\n",
      "data/email/ham\\19.txt\n",
      "LinkedIn\n",
      "\n",
      "Julius O requested to add you as a connection on LinkedIn:\n",
      "\n",
      "Hi Peter.\n",
      "\n",
      "Looking forward to the book!\n",
      "\n",
      " \n",
      "Accept \tView invitation from Julius O\n",
      "\n",
      "data/email/ham\\2.txt\n",
      "Yay to you both doing fine!\n",
      "\n",
      "I'm working on an MBA in Design Strategy at CCA (top art school.)  It's a new program focusing on more of a right-brained creative and strategic approach to management.  I'm an 1/8 of the way done today!\n",
      "data/email/ham\\20.txt\n",
      "I've thought about this and think it's possible. We should get another\n",
      "lunch. I have a car now and could come pick you up this time. Does\n",
      "this wednesday work? 11:50?\n",
      "\n",
      "Can I have a signed copy of you book?\n",
      "data/email/ham\\21.txt\n",
      "we saw this on the way to the coast...thought u might like it\n",
      "\n",
      "hangzhou is huge, one day wasn't enough, but we got a glimpse...\n",
      "\n",
      "we went inside the china pavilion at expo, it is pretty interesting,\n",
      "each province has an exhibit...\n",
      "data/email/ham\\22.txt\n",
      "Hi Hommies,\n",
      "\n",
      "Just got a phone call from the roofer, they will come and spaying the foaming today. it will be dusty. pls close all the doors and windows.\n",
      "Could you help me to close my bathroom window, cat window and the sliding door behind the TV?\n",
      "I don't know how can those 2 cats survive......\n",
      "\n",
      "Sorry for any inconvenience!\n",
      "data/email/ham\\23.txt\n",
      "SciFinance now automatically generates GPU-enabled pricing & risk model source code that runs up to 50-300x faster than serial code using a new NVIDIA Fermi-class Tesla 20-Series GPU.\n",
      "\n",
      "SciFinance?is a derivatives pricing and risk model development tool that automatically generates C/C++ and GPU-enabled source code from concise, high-level model specifications. No parallel computing or CUDA programming expertise is required.\n",
      "\n",
      "SciFinance's automatic, GPU-enabled Monte Carlo pricing model source code generation capabilities have been significantly extended in the latest release. This includes:\n",
      "\n",
      "data/email/ham\\24.txt\n",
      "Ok I will be there by 10:00 at the latest.\n",
      "data/email/ham\\25.txt\n",
      "That is cold.  Is there going to be a retirement party?  \n",
      "Are the leaves changing color?\n",
      "data/email/ham\\3.txt\n",
      "WHat is going on there?\n",
      "I talked to John on email.  We talked about some computer stuff that's it.\n",
      "\n",
      "I went bike riding in the rain, it was not that cold.\n",
      "\n",
      "We went to the museum in SF yesterday it was $3 to get in and they had\n",
      "free food.  At the same time was a SF Giants game, when we got done we\n",
      "had to take the train with all the Giants fans, they are 1/2 drunk.\n",
      "data/email/ham\\4.txt\n",
      "Yo.  I've been working on my running website.  I'm using jquery and the jqplot plugin.  I'm not too far away from having a prototype to launch.  \n",
      "\n",
      "You used jqplot right?  If not, I think you would like it.\n",
      "data/email/ham\\5.txt\n",
      "There was a guy at the gas station who told me that if I knew Mandarin\n",
      "and Python I could get a job with the FBI.\n",
      "data/email/ham\\6.txt\n"
     ]
    },
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0x92 in position 884: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(file_path, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(file_path)\n\u001b[1;32m----> 6\u001b[0m     a \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(a)\n",
      "File \u001b[1;32mC:\\Python-3107\\lib\\codecs.py:322\u001b[0m, in \u001b[0;36mBufferedIncrementalDecoder.decode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m    319\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;66;03m# decode input (taking the buffer into account)\u001b[39;00m\n\u001b[0;32m    321\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer \u001b[38;5;241m+\u001b[39m \u001b[38;5;28minput\u001b[39m\n\u001b[1;32m--> 322\u001b[0m     (result, consumed) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_buffer_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    323\u001b[0m     \u001b[38;5;66;03m# keep undecoded input until the next call\u001b[39;00m\n\u001b[0;32m    324\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer \u001b[38;5;241m=\u001b[39m data[consumed:]\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0x92 in position 884: invalid start byte"
     ]
    }
   ],
   "source": [
    "ham = []\n",
    "for filename in os.listdir(ham_path):\n",
    "    file_path = os.path.join(ham_path, filename)\n",
    "    with open(file_path, encoding=\"utf-8\") as f:\n",
    "        print(file_path)\n",
    "        a = f.read()\n",
    "        print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0a1be16e-f627-49e1-9658-178156910fdb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def loadDataSet():\n",
    "    postingList=[['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],\n",
    "                 ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],\n",
    "                 ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],\n",
    "                 ['stop', 'posting', 'stupid', 'worthless', 'garbage'],\n",
    "                 ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],\n",
    "                 ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]\n",
    "    classVec = np.array([0,1,0,1,0,1])    #1 is abusive, 0 not\n",
    "    return postingList, classVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d26ab54a-acd1-479b-8e13-8af49f9a369e",
   "metadata": {},
   "outputs": [],
   "source": [
    "postingList,listClasses = loadDataSet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f9e6117b-0966-4805-9d5f-287f7a0b0cd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],\n",
       " ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],\n",
       " ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],\n",
       " ['stop', 'posting', 'stupid', 'worthless', 'garbage'],\n",
       " ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],\n",
       " ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "postingList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8584d3f8-71f8-4ec0-be68-5a1488f86942",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 1, 0, 1])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listClasses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc3979a-582b-4c98-bf98-67d05a41b4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract unique text\n",
    "def createVocabList(dataSet):\n",
    "    vocab_set = []\n",
    "    for post in dataSet:\n",
    "        vocab_set += post\n",
    "    return list(set(vocab_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b1e817-4e30-4eac-a8ea-c61e3fb95b20",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['licks', 'take', 'mr', 'not', 'ate', 'maybe', 'I', 'quit', 'worthless', 'park', 'to', 'love', 'steak', 'dog', 'stop', 'posting', 'garbage', 'has', 'food', 'problems', 'buying', 'my', 'how', 'please', 'flea', 'help', 'stupid', 'is', 'so', 'him', 'dalmation', 'cute']\n"
     ]
    }
   ],
   "source": [
    "vocabList = createVocabList(postingList)\n",
    "print(vocabList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6d3397-2d43-4bc6-8ef6-b4024c63450c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transfer text into number and 1 = exist, 0 = not exist\n",
    "def setOfWords2Vec(vocabList, inputSet):\n",
    "    returnVec = [0]*len(vocabList)\n",
    "    for word in inputSet:\n",
    "        if word in vocabList:\n",
    "            returnVec[vocabList.index(word)] = 1\n",
    "    return returnVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc532a3-1a43-4f03-8049-5e5821906bc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(setOfWords2Vec(vocabList, postingList[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db342e08-bbc3-437c-bd1b-8c17687d72fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 1, 0, 0])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "postingList,listClasses = loadDataSet()\n",
    "vocabList = createVocabList(postingList)\n",
    "trainMat = []\n",
    "for doc in postingList:\n",
    "    trainMat.append(setOfWords2Vec(vocabList, doc))\n",
    "trainMat = np.array(trainMat)\n",
    "trainMat[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09f2830-212e-4232-82e6-7ade64c58ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Naive_bayes(X, y, y_value):\n",
    "    X = X + 1\n",
    "    prob_y = np.count_nonzero(y == y_value) / len(y)    # P(y)\n",
    "    class_index = np.where(y == y_value)\n",
    "    group = X[class_index]\n",
    "    prob_x = group.sum(axis=0) / group.sum()    # P(X|y)\n",
    "    return prob_x, prob_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc86f1a-3fd1-43a2-b1a7-3daf2bd5c2c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.02608696, 0.03478261, 0.02608696, 0.03478261, 0.02608696,\n",
       "        0.03478261, 0.02608696, 0.03478261, 0.04347826, 0.03478261,\n",
       "        0.03478261, 0.02608696, 0.02608696, 0.04347826, 0.03478261,\n",
       "        0.03478261, 0.03478261, 0.02608696, 0.03478261, 0.02608696,\n",
       "        0.03478261, 0.02608696, 0.02608696, 0.02608696, 0.02608696,\n",
       "        0.02608696, 0.05217391, 0.02608696, 0.02608696, 0.03478261,\n",
       "        0.02608696, 0.02608696]),\n",
       " 0.5)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for y_value in np.unique(listClasses):\n",
    "    prob_x, prob_y = Naive_bayes(trainMat, listClasses, y_value)\n",
    "prob_x, prob_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eaf2353",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
