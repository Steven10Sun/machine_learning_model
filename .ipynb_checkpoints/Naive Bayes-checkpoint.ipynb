{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad8e022c-95a2-430e-b2db-ca237285b0c2",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797f92e4-504c-4fb7-95ce-5391aa1063bb",
   "metadata": {},
   "source": [
    "### Model\n",
    "#### $P(y|X)=\\frac{P(X|y)P(y)}{P(X)}$\n",
    "\n",
    "if you have a lot of variables: <br>\n",
    "#### $P(y|x_1, ..., x_n)=\\frac{P(x_1|y)P(x_2|y)...P(x_n|y)P(y)}{P(x_1)P(x_2)...P(x_n)}$\n",
    "\n",
    "__*!!! since the denominator is the same for both probabilities, it can be omitted from the calculation, and only need to consider the numerator.*__\n",
    "###### $P(y|x_1, ..., x_n)=P(x_1|y)P(x_2|y)...P(x_n|y)P(y)$ \n",
    "<br>\n",
    "Naive Bayes is to use variable X to classify target y based on comparasion of probability of being target 1, target 2, target n\n",
    "\n",
    "### Assumption\n",
    "1. Features are independent to each other. \n",
    "2. Every feature is equally important.\n",
    "\n",
    "**Step**\n",
    "1. separate the dataset by target, you need to have a value count table for each class\n",
    "2. calculate the probability of each target: $P(Y)$ = number of Y / total number of entities\n",
    "3. for loop each target group:\n",
    "    * sum up the frequency for each unique word\n",
    "    * calculate the probability of each word in the target group, **remark: it is $P(X|y)$, conditional prob of x given by target y**\n",
    "    \n",
    "Now, we have $P(Y)$, $P(X|y)$ and P(X)<br>\n",
    "**Input new data** <br>\n",
    "for loop each target group:\n",
    "1. multiple the conditional probability for the input data: $P(y|x_1, ..., x_n)=\\frac{P(x_1|y)P(x_2|y)...P(x_n|y)P(y)}{P(x_1)P(x_2)...P(x_n)}$\n",
    "2. compare probability (likelihood) and assign the target with highest prob to the input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b1eac51-940b-453a-90a0-66301b705f2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd3c6e4b-9a60-46f8-a582-3d1928e416af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ham_word_list = []\n",
    "spam_word_list = []\n",
    "for i in range(1, 26):\n",
    "    ham_file = open(f'data/email/ham/{i}.txt', 'r').read()\n",
    "    ham_words = list(filter(None, re.split(r\"\\W+\", ham_file)))\n",
    "    ham_words = [j.lower() for j in ham_words if len(j) > 2]\n",
    "    ham_word_list.extend(ham_words)\n",
    "    \n",
    "    spam_file = open(f'data/email/spam/{i}.txt', 'r').read()\n",
    "    spam_words = list(filter(None, re.split(r\"\\W+\", spam_file)))\n",
    "    spam_words = [j.lower() for j in spam_words if len(j) > 2]\n",
    "    spam_word_list.extend(spam_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85d36a2a-c8b0-4b03-a290-e411dba8678a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ham_word_list = []\n",
    "spam_word_list = []\n",
    "ham_file = open(f'data/email/ham/full.txt', 'r', encoding='utf-8').read()\n",
    "ham_words = list(filter(None, re.split(r\"\\W+\", ham_file)))\n",
    "ham_words = [j.lower() for j in ham_words if len(j) > 2]\n",
    "ham_word_list.extend(ham_words)\n",
    "\n",
    "spam_file = open(f'data/email/spam/spam_full.txt', 'r', encoding='utf-8').read()\n",
    "spam_words = list(filter(None, re.split(r\"\\W+\", spam_file)))\n",
    "spam_words = [j.lower() for j in spam_words if len(j) > 2]\n",
    "spam_word_list.extend(spam_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf47b90a-31d4-4ce3-898a-cab6b4f509af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "value_count = Counter(ham_word_list)\n",
    "ham_df = pd.DataFrame.from_dict(value_count, orient='index', columns=['count']).reset_index()\n",
    "ham_df = ham_df.rename(columns={'index': 'word'})\n",
    "\n",
    "value_count = Counter(spam_word_list)\n",
    "spam_df = pd.DataFrame.from_dict(value_count, orient='index', columns=['count']).reset_index()\n",
    "spam_df = spam_df.rename(columns={'index': 'word'})\n",
    "\n",
    "ham_df['CP'] = ham_df['count'] / sum(ham_df['count'])\n",
    "spam_df['CP'] = spam_df['count'] / sum(spam_df['count'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e97cfaa-1094-4d2c-b182-bde2859ec601",
   "metadata": {},
   "source": [
    "# --------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "4d0bf9fe-1d41-41d1-be69-36d28c8407cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def loadDataSet():\n",
    "    postingList=[['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],\n",
    "                 ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],\n",
    "                 ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],\n",
    "                 ['stop', 'posting', 'stupid', 'worthless', 'garbage'],\n",
    "                 ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],\n",
    "                 ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]\n",
    "    classVec = [0,1,0,1,0,1]    #1 is abusive, 0 not\n",
    "    return postingList,classVec\n",
    "\n",
    "# create a unique word list\n",
    "def create_vocab_list(data_set):\n",
    "    vocab_set = set()  # create empty set\n",
    "    for document in data_set:\n",
    "        vocab_set = vocab_set | set(document)  # union of the two sets\n",
    "    return list(vocab_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "724008bd-4079-4fd1-b012-5b2d9f5aa135",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Conditional_Prob(create_vocab_list, data_set, class_set):\n",
    "    return_vec_0 = np.ones(len(create_vocab_list))\n",
    "    return_vec_1 = np.ones(len(create_vocab_list))\n",
    "    for num in range(len(class_set)):\n",
    "        if class_set[num] == 0:\n",
    "            for word in data_set[num]:\n",
    "                return_vec_0[create_vocab_list.index(word)] += 1\n",
    "        if class_set[num] == 1: \n",
    "            for word in data_set[num]:\n",
    "                return_vec_1[create_vocab_list.index(word)] += 1\n",
    "    P_0 = Counter(class_set)[0] / len(class_set)\n",
    "    P_1 = 1 - P_0\n",
    "    CP_0 = return_vec_0 / np.sum(return_vec_0)\n",
    "    CP_1 = return_vec_1 / np.sum(return_vec_1)\n",
    "    return P_0, P_1, CP_0, CP_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "a8457788-1c3c-4fa4-92d8-45b886abb1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set, class_set = loadDataSet()\n",
    "vocab_list = create_vocab_list(data_set)\n",
    "word_count = len(vocab_list)\n",
    "P_0, P_1, CP_0, CP_1 = Conditional_Prob(vocab_list, data_set, class_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "b6cbe712-3d18-4d00-8fa0-e0fd7fca36c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Classify(vocab_list, P_0, P_1, CP_0, CP_1, test):\n",
    "    test_index = [vocab_list.index(i) for i in test if i in vocab_list]\n",
    "    p0 = np.sum(np.log(CP_0[test_index]))*P_0\n",
    "    p1 = np.sum(np.log(CP_1[test_index]))*P_1\n",
    "    if p0 > p1:\n",
    "        return '0'\n",
    "    else:\n",
    "        return '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "bc047a91-398b-4bca-aa7b-3598f4ea617c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "root_dir = \"data/email\"\n",
    "for sub_folder in os.listdir(root_dir):\n",
    "    for txt_file in os.listdir(f'{root_dir}/{sub_folder}'):\n",
    "        with open(f'{root_dir}/{sub_folder}/{txt_file}', encoding=\"latin-1\") as f:\n",
    "            file_content = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c534431-2908-4b33-bfe9-b7a1e3ef228f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "root_dir = 'data/email'\n",
    "for "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "b6f781b9-8743-487f-a306-7aca5c24e26c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi Peter,\\n\\nWith Jose out of town, do you want to\\nmeet once in a while to keep things\\ngoing and do some interesting stuff?\\n\\nLet me know\\nEugene',\n",
       " \"Yay to you both doing fine!\\n\\nI'm working on an MBA in Design Strategy at CCA (top art school.)  It's a new program focusing on more of a right-brained creative and strategic approach to management.  I'm an 1/8 of the way done today!\",\n",
       " \"WHat is going on there?\\nI talked to John on email.  We talked about some computer stuff that's it.\\n\\nI went bike riding in the rain, it was not that cold.\\n\\nWe went to the museum in SF yesterday it was $3 to get in and they had\\nfree food.  At the same time was a SF Giants game, when we got done we\\nhad to take the train with all the Giants fans, they are 1/2 drunk.\",\n",
       " \"Yo.  I've been working on my running website.  I'm using jquery and the jqplot plugin.  I'm not too far away from having a prototype to launch.  \\n\\nYou used jqplot right?  If not, I think you would like it.\",\n",
       " 'There was a guy at the gas station who told me that if I knew Mandarin\\nand Python I could get a job with the FBI.',\n",
       " 'Hello,\\n\\nSince you are an owner of at least one Google Groups group that uses the customized welcome message, pages or files, we are writing to inform you that we will no longer be supporting these features starting February 2011. We made this decision so that we can focus on improving the core functionalities of Google Groups -- mailing lists and forum discussions.  Instead of these features, we encourage you to use products that are designed specifically for file storage and page creation, such as Google Docs and Google Sites.\\n\\nFor example, you can easily create your pages on Google Sites and share the site (http://www.google.com/support/sites/bin/answer.py?hl=en&answer=174623) with the members of your group. You can also store your files on the site by attaching files to pages (http://www.google.com/support/sites/bin/answer.py?hl=en&answer=90563) on the site. If you\\x92re just looking for a place to upload your files so that your group members can download them, we suggest you try Google Docs. You can upload files (http://docs.google.com/support/bin/answer.py?hl=en&answer=50092) and share access with either a group (http://docs.google.com/support/bin/answer.py?hl=en&answer=66343) or an individual (http://docs.google.com/support/bin/answer.py?hl=en&answer=86152), assigning either edit or download only access to the files.\\n\\nyou have received this mandatory email service announcement to update you about important changes to Google Groups.',\n",
       " 'Zach Hamm commented on your status.\\n\\nZach wrote:\\n\"doggy style - enough said, thank you & good night\"\\n\\n\\n',\n",
       " 'This e-mail was sent from a notification-only address that cannot accept incoming e-mail. Please do not reply to this message.\\n\\nThank you for your online reservation. The store you selected has located the item you requested and has placed it on hold in your name. Please note that all items are held for 1 day.  Please note store prices may differ from those online.\\n\\nIf you have questions or need assistance with your reservation, please contact the store at the phone number listed below. You can also access store information, such as store hours and location, on the web at http://www.borders.com/online/store/StoreDetailView_98.',\n",
       " \"Hi Peter,\\n\\nThese are the only good scenic ones and it's too bad there was a girl's back in one of them. Just try to enjoy the blue sky : ))\\n\\nD\",\n",
       " 'Ryan Whybrew commented on your status.\\n\\nRyan wrote:\\n\"turd ferguson or butt horn.\"\\n',\n",
       " 'Arvind Thirumalai commented on your status.\\n\\nArvind wrote:\\n\"\"you know\"\"\\n\\n\\nReply to this email to comment on this status.\\n\\n',\n",
       " \"Thanks Peter.\\n\\nI'll definitely check in on this. How is your book\\ngoing? I heard chapter 1 came in and it was in \\ngood shape. ;-)\\n\\nI hope you are doing well.\\n\\nCheers,\\n\\nTroy\",\n",
       " 'Jay Stepp commented on your status.\\n\\nJay wrote:\\n\"\"to the\" ???\"\\n\\n\\nReply to this email to comment on this status.\\n\\nTo see the comment thread, follow the link below:\\n\\n',\n",
       " \"LinkedIn\\n\\nKerry Haloney requested to add you as a connection on LinkedIn:\\n\\nPeter,\\n\\nI'd like to add you to my professional network on LinkedIn.\\n\\n- Kerry Haloney\\n \\n\",\n",
       " 'Hi Peter,\\n \\nThe hotels are the ones that rent out the tent. They are all lined up on the hotel grounds : )) So much for being one with nature, more like being one with a couple dozen tour groups and nature.\\nI have about 100M of pictures from that trip. I can go through them and get you jpgs of my favorite scenic pictures.\\n \\nWhere are you and Jocelyn now? New York? Will you come to Tokyo for Chinese New Year? Perhaps to see the two of you then. I will go to Thailand for winter holiday to see my mom : )\\n \\nTake care,\\nD\\n',\n",
       " 'yeah I am ready.  I may not be here because Jar Jar has plane tickets to Germany for me.  ',\n",
       " 'Benoit Mandelbrot 1924-2010\\n\\nBenoit Mandelbrot 1924-2010\\n\\nWilmott Team\\n\\nBenoit Mandelbrot, the mathematician, the father of fractal mathematics, and advocate of more sophisticated modelling in quantitative finance, died on 14th October 2010 aged 85.\\n\\nWilmott magazine has often featured Mandelbrot, his ideas, and the work of others inspired by his fundamental insights.\\n\\nYou must be logged on to view these articles from past issues of Wilmott Magazine.',\n",
       " 'Hi Peter,\\n\\n    Sure thing.  Sounds good.  Let me know what time would be good for you.\\nI will come prepared with some ideas and we can go from there.\\n\\nRegards,\\n\\n-Vivek.',\n",
       " 'LinkedIn\\n\\nJulius O requested to add you as a connection on LinkedIn:\\n\\nHi Peter.\\n\\nLooking forward to the book!\\n\\n \\nAccept \\tView invitation from Julius O\\n',\n",
       " \"I've thought about this and think it's possible. We should get another\\nlunch. I have a car now and could come pick you up this time. Does\\nthis wednesday work? 11:50?\\n\\nCan I have a signed copy of you book?\",\n",
       " \"we saw this on the way to the coast...thought u might like it\\n\\nhangzhou is huge, one day wasn't enough, but we got a glimpse...\\n\\nwe went inside the china pavilion at expo, it is pretty interesting,\\neach province has an exhibit...\",\n",
       " \"Hi Hommies,\\n\\nJust got a phone call from the roofer, they will come and spaying the foaming today. it will be dusty. pls close all the doors and windows.\\nCould you help me to close my bathroom window, cat window and the sliding door behind the TV?\\nI don't know how can those 2 cats survive......\\n\\nSorry for any inconvenience!\",\n",
       " \"SciFinance now automatically generates GPU-enabled pricing & risk model source code that runs up to 50-300x faster than serial code using a new NVIDIA Fermi-class Tesla 20-Series GPU.\\n\\nSciFinance?is a derivatives pricing and risk model development tool that automatically generates C/C++ and GPU-enabled source code from concise, high-level model specifications. No parallel computing or CUDA programming expertise is required.\\n\\nSciFinance's automatic, GPU-enabled Monte Carlo pricing model source code generation capabilities have been significantly extended in the latest release. This includes:\\n\",\n",
       " 'Ok I will be there by 10:00 at the latest.',\n",
       " 'That is cold.  Is there going to be a retirement party?  \\nAre the leaves changing color?']"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "root_dir = \"data/email\"\n",
    "\n",
    "for folder in os.listdir(root_dir):\n",
    "    if os.path.isdir(os.path.join(root_dir, folder)):\n",
    "        # This is a subdirectory\n",
    "        for file in os.listdir(os.path.join(root_dir, folder)):\n",
    "            if file.endswith(\".txt\"):\n",
    "                # This is a text file\n",
    "                file_path = os.path.join(root_dir, folder, file)\n",
    "                # Do something with the file path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "676b868b-308b-4250-91bf-dc08d490a02b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.txt\n",
      "10.txt\n",
      "11.txt\n",
      "12.txt\n",
      "13.txt\n",
      "14.txt\n",
      "15.txt\n",
      "16.txt\n",
      "17.txt\n",
      "18.txt\n",
      "19.txt\n",
      "2.txt\n",
      "20.txt\n",
      "21.txt\n",
      "22.txt\n",
      "23.txt\n",
      "24.txt\n",
      "25.txt\n",
      "3.txt\n",
      "4.txt\n",
      "5.txt\n",
      "6.txt\n",
      "7.txt\n",
      "8.txt\n",
      "9.txt\n",
      "full.txt\n",
      "1.txt\n",
      "10.txt\n",
      "11.txt\n",
      "12.txt\n",
      "13.txt\n",
      "14.txt\n",
      "15.txt\n",
      "16.txt\n",
      "17.txt\n",
      "18.txt\n",
      "19.txt\n",
      "2.txt\n",
      "20.txt\n",
      "21.txt\n",
      "22.txt\n",
      "23.txt\n",
      "24.txt\n",
      "25.txt\n",
      "3.txt\n",
      "4.txt\n",
      "5.txt\n",
      "6.txt\n",
      "7.txt\n",
      "8.txt\n",
      "9.txt\n",
      "spam_full.txt\n"
     ]
    }
   ],
   "source": [
    "root_dir = \"data/email\"\n",
    "for sub_folder in os.listdir(root_dir):\n",
    "    for i in os.listdir(f'{root_dir}/{sub_folder}'):\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a787d3-b110-4d74-843e-88c9dd396198",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950a30e2-4546-4018-8319-dcb2ebb0f465",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a227e37-c936-4a44-a513-9e1f09f21dcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41cfa802-8033-4106-8a51-787c77aacb0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9821aa0-6634-4d37-9cdb-674fd2ad250e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "0c64a78b-af64-4039-ab2c-2a2df3f9a7ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1'"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def load_data(folder):\n",
    "    data = []\n",
    "    \n",
    "    for i in range(1, 26):\n",
    "        with open(f\"{folder}/{i}.txt\", encoding=\"latin-1\") as f:\n",
    "            words = text_parse(f.read())\n",
    "            data.append(words)\n",
    "            \n",
    "    return data\n",
    "\n",
    "\n",
    "def split_data(data, train_size=0.8):\n",
    "    train_set = []\n",
    "    test_set = []\n",
    "    train_classes = []\n",
    "    test_classes = []\n",
    "    n_train = int(len(data) * train_size)\n",
    "    \n",
    "    for i, doc in enumerate(data):\n",
    "        if i < n_train:\n",
    "            train_set.append(doc)\n",
    "            train_classes.append(1 if i < n_train/2 else 0)\n",
    "        else:\n",
    "            test_set.append(doc)\n",
    "            test_classes.append(1 if i < (n_train+len(data))/2 else 0)\n",
    "            \n",
    "    return train_set, train_classes, test_set, test_classes\n",
    "\n",
    "\n",
    "def train_and_evaluate(train_set, train_classes, test_set, test_classes):\n",
    "    vocab_list = create_vocab_list(train_set)\n",
    "    train_mat = [bag_of_words_2_vec_mn(vocab_list, doc) for doc in train_set]\n",
    "    train_classes = np.array(train_classes)\n",
    "    test_mat = [bag_of_words_2_vec_mn(vocab_list, doc) for doc in test_set]\n",
    "    \n",
    "    p0v, p1v, p_spam = train_nb0(np.array(train_mat), train_classes)\n",
    "    \n",
    "    error_count = sum(classify_nb(np.array(test_mat), p0v, p1v, p_spam) != np.array(test_classes))\n",
    "    error_rate = error_count/len(test_set)\n",
    "    \n",
    "    print(f\"The error rate is: {error_rate:.2%}\")\n",
    "    \n",
    "    return vocab_list\n",
    "\n",
    "\n",
    "def spam_test():\n",
    "    spam_data = load_data(\"email/spam\")\n",
    "    ham_data = load_data(\"email/ham\")\n",
    "    data = spam_data + ham_data\n",
    "    train_set, train_classes, test_set, test_classes = split_data(data)\n",
    "    vocab_list = train_and_evaluate(train_set, train_classes, test_set, test_classes)\n",
    "    \n",
    "    return vocab_list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
