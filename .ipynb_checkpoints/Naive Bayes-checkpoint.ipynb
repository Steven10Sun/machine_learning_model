{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad8e022c-95a2-430e-b2db-ca237285b0c2",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797f92e4-504c-4fb7-95ce-5391aa1063bb",
   "metadata": {},
   "source": [
    "### Model\n",
    "#### $P(y|X)=\\frac{P(X|y)P(y)}{P(X)}$\n",
    "\n",
    "if you have a lot of variables: <br>\n",
    "#### $P(y|x_1, ..., x_n)=\\frac{P(x_1|y)P(x_2|y)...P(x_n|y)P(y)}{P(x_1)P(x_2)...P(x_n)}$\n",
    "\n",
    "__*!!! since the denominator is the same for both probabilities, it can be omitted from the calculation, and only need to consider the numerator.*__\n",
    "###### $P(y|x_1, ..., x_n)=P(x_1|y)P(x_2|y)...P(x_n|y)P(y)$ \n",
    "<br>\n",
    "Naive Bayes is to use variable X to classify target y based on comparasion of probability of being target 1, target 2, target n\n",
    "\n",
    "### Assumption\n",
    "1. Features are independent to each other. \n",
    "2. Every feature is equally important.\n",
    "\n",
    "**Step**\n",
    "1. separate the dataset by target\n",
    "2. calculate the probability of each target: $P(Y)$\n",
    "3. for loop each target group:\n",
    "    * sum up the frequency for each unique word\n",
    "    * calculate the probability of each word in the target group, **remark: it is $P(X|y)$, conditional prob of x given by target y**\n",
    "    \n",
    "Now, we have $P(Y)$, $P(X|y)$ and P(X)<br>\n",
    "**Input new data** <br>\n",
    "for loop each target group:\n",
    "1. multiple the conditional probability for the input data: $P(y|x_1, ..., x_n)=\\frac{P(x_1|y)P(x_2|y)...P(x_n|y)P(y)}{P(x_1)P(x_2)...P(x_n)}$\n",
    "2. compare probability (likelihood) and assign the target with highest prob to the input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b1eac51-940b-453a-90a0-66301b705f2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a23e83cd-093d-4c4c-bba9-9cc5ec8b1dad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi Peter,\n",
      "\n",
      "With Jose out of town, do you want to\n",
      "meet once in a while to keep things\n",
      "going and do some interesting stuff?\n",
      "\n",
      "Let me know\n",
      "Eugene\n",
      "Ryan Whybrew commented on your status.\n",
      "\n",
      "Ryan wrote:\n",
      "\"turd ferguson or butt horn.\"\n",
      "\n",
      "Arvind Thirumalai commented on your status.\n",
      "\n",
      "Arvind wrote:\n",
      "\"\"you know\"\"\n",
      "\n",
      "\n",
      "Reply to this email to comment on this status.\n",
      "\n",
      "\n",
      "Thanks Peter.\n",
      "\n",
      "I'll definitely check in on this. How is your book\n",
      "going? I heard chapter 1 came in and it was in \n",
      "good shape. ;-)\n",
      "\n",
      "I hope you are doing well.\n",
      "\n",
      "Cheers,\n",
      "\n",
      "Troy\n",
      "Jay Stepp commented on your status.\n",
      "\n",
      "Jay wrote:\n",
      "\"\"to the\" ???\"\n",
      "\n",
      "\n",
      "Reply to this email to comment on this status.\n",
      "\n",
      "To see the comment thread, follow the link below:\n",
      "\n",
      "\n",
      "LinkedIn\n",
      "\n",
      "Kerry Haloney requested to add you as a connection on LinkedIn:\n",
      "\n",
      "Peter,\n",
      "\n",
      "I'd like to add you to my professional network on LinkedIn.\n",
      "\n",
      "- Kerry Haloney\n",
      " \n",
      "\n",
      "Hi Peter,\n",
      " \n",
      "The hotels are the ones that rent out the tent. They are all lined up on the hotel grounds : )) So much for being one with nature, more like being one with a couple dozen tour groups and nature.\n",
      "I have about 100M of pictures from that trip. I can go through them and get you jpgs of my favorite scenic pictures.\n",
      " \n",
      "Where are you and Jocelyn now? New York? Will you come to Tokyo for Chinese New Year? Perhaps to see the two of you then. I will go to Thailand for winter holiday to see my mom : )\n",
      " \n",
      "Take care,\n",
      "D\n",
      "\n",
      "yeah I am ready.  I may not be here because Jar Jar has plane tickets to Germany for me.  \n",
      "Benoit Mandelbrot 1924-2010\n",
      "\n",
      "Benoit Mandelbrot 1924-2010\n",
      "\n",
      "Wilmott Team\n",
      "\n",
      "Benoit Mandelbrot, the mathematician, the father of fractal mathematics, and advocate of more sophisticated modelling in quantitative finance, died on 14th October 2010 aged 85.\n",
      "\n",
      "Wilmott magazine has often featured Mandelbrot, his ideas, and the work of others inspired by his fundamental insights.\n",
      "\n",
      "You must be logged on to view these articles from past issues of Wilmott Magazine.\n",
      "Hi Peter,\n",
      "\n",
      "    Sure thing.  Sounds good.  Let me know what time would be good for you.\n",
      "I will come prepared with some ideas and we can go from there.\n",
      "\n",
      "Regards,\n",
      "\n",
      "-Vivek.\n",
      "LinkedIn\n",
      "\n",
      "Julius O requested to add you as a connection on LinkedIn:\n",
      "\n",
      "Hi Peter.\n",
      "\n",
      "Looking forward to the book!\n",
      "\n",
      " \n",
      "Accept \tView invitation from Julius O\n",
      "\n",
      "Yay to you both doing fine!\n",
      "\n",
      "I'm working on an MBA in Design Strategy at CCA (top art school.)  It's a new program focusing on more of a right-brained creative and strategic approach to management.  I'm an 1/8 of the way done today!\n",
      "I've thought about this and think it's possible. We should get another\n",
      "lunch. I have a car now and could come pick you up this time. Does\n",
      "this wednesday work? 11:50?\n",
      "\n",
      "Can I have a signed copy of you book?\n",
      "we saw this on the way to the coast...thought u might like it\n",
      "\n",
      "hangzhou is huge, one day wasn't enough, but we got a glimpse...\n",
      "\n",
      "we went inside the china pavilion at expo, it is pretty interesting,\n",
      "each province has an exhibit...\n",
      "Hi Hommies,\n",
      "\n",
      "Just got a phone call from the roofer, they will come and spaying the foaming today. it will be dusty. pls close all the doors and windows.\n",
      "Could you help me to close my bathroom window, cat window and the sliding door behind the TV?\n",
      "I don't know how can those 2 cats survive......\n",
      "\n",
      "Sorry for any inconvenience!\n",
      "SciFinance now automatically generates GPU-enabled pricing & risk model source code that runs up to 50-300x faster than serial code using a new NVIDIA Fermi-class Tesla 20-Series GPU.\n",
      "\n",
      "SciFinance?is a derivatives pricing and risk model development tool that automatically generates C/C++ and GPU-enabled source code from concise, high-level model specifications. No parallel computing or CUDA programming expertise is required.\n",
      "\n",
      "SciFinance's automatic, GPU-enabled Monte Carlo pricing model source code generation capabilities have been significantly extended in the latest release. This includes:\n",
      "\n",
      "Ok I will be there by 10:00 at the latest.\n",
      "That is cold.  Is there going to be a retirement party?  \n",
      "Are the leaves changing color?\n",
      "WHat is going on there?\n",
      "I talked to John on email.  We talked about some computer stuff that's it.\n",
      "\n",
      "I went bike riding in the rain, it was not that cold.\n",
      "\n",
      "We went to the museum in SF yesterday it was $3 to get in and they had\n",
      "free food.  At the same time was a SF Giants game, when we got done we\n",
      "had to take the train with all the Giants fans, they are 1/2 drunk.\n",
      "Yo.  I've been working on my running website.  I'm using jquery and the jqplot plugin.  I'm not too far away from having a prototype to launch.  \n",
      "\n",
      "You used jqplot right?  If not, I think you would like it.\n",
      "There was a guy at the gas station who told me that if I knew Mandarin\n",
      "and Python I could get a job with the FBI.\n",
      "Hello,\n",
      "\n",
      "Since you are an owner of at least one Google Groups group that uses the customized welcome message, pages or files, we are writing to inform you that we will no longer be supporting these features starting February 2011. We made this decision so that we can focus on improving the core functionalities of Google Groups -- mailing lists and forum discussions.  Instead of these features, we encourage you to use products that are designed specifically for file storage and page creation, such as Google Docs and Google Sites.\n",
      "\n",
      "For example, you can easily create your pages on Google Sites and share the site (http://www.google.com/support/sites/bin/answer.py?hl=en&answer=174623) with the members of your group. You can also store your files on the site by attaching files to pages (http://www.google.com/support/sites/bin/answer.py?hl=en&answer=90563) on the site. If youâ€™re just looking for a place to upload your files so that your group members can download them, we suggest you try Google Docs. You can upload files (http://docs.google.com/support/bin/answer.py?hl=en&answer=50092) and share access with either a group (http://docs.google.com/support/bin/answer.py?hl=en&answer=66343) or an individual (http://docs.google.com/support/bin/answer.py?hl=en&answer=86152), assigning either edit or download only access to the files.\n",
      "\n",
      "you have received this mandatory email service announcement to update you about important changes to Google Groups.\n",
      "Zach Hamm commented on your status.\n",
      "\n",
      "Zach wrote:\n",
      "\"doggy style - enough said, thank you & good night\"\n",
      "\n",
      "\n",
      "\n",
      "This e-mail was sent from a notification-only address that cannot accept incoming e-mail. Please do not reply to this message.\n",
      "\n",
      "Thank you for your online reservation. The store you selected has located the item you requested and has placed it on hold in your name. Please note that all items are held for 1 day.  Please note store prices may differ from those online.\n",
      "\n",
      "If you have questions or need assistance with your reservation, please contact the store at the phone number listed below. You can also access store information, such as store hours and location, on the web at http://www.borders.com/online/store/StoreDetailView_98.\n",
      "Hi Peter,\n",
      "\n",
      "These are the only good scenic ones and it's too bad there was a girl's back in one of them. Just try to enjoy the blue sky : ))\n",
      "\n",
      "D\n"
     ]
    }
   ],
   "source": [
    "ham_path = 'data\\email\\ham'\n",
    "spam_path = 'data/email/spam/'\n",
    "\n",
    "ham = []\n",
    "for filename in os.listdir(ham_path):\n",
    "    file_path = os.path.join(ham_path, filename)\n",
    "    with open(file_path, encoding='cp1252') as f:\n",
    "        a = f.read()\n",
    "        print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0a1be16e-f627-49e1-9658-178156910fdb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def loadDataSet():\n",
    "    postingList=[['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],\n",
    "                 ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],\n",
    "                 ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],\n",
    "                 ['stop', 'posting', 'stupid', 'worthless', 'garbage'],\n",
    "                 ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],\n",
    "                 ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]\n",
    "    classVec = np.array([0,1,0,1,0,1])    #1 is abusive, 0 not\n",
    "    return postingList, classVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d26ab54a-acd1-479b-8e13-8af49f9a369e",
   "metadata": {},
   "outputs": [],
   "source": [
    "postingList,listClasses = loadDataSet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f9e6117b-0966-4805-9d5f-287f7a0b0cd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],\n",
       " ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],\n",
       " ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],\n",
       " ['stop', 'posting', 'stupid', 'worthless', 'garbage'],\n",
       " ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],\n",
       " ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "postingList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8584d3f8-71f8-4ec0-be68-5a1488f86942",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 1, 0, 1])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listClasses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc3979a-582b-4c98-bf98-67d05a41b4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract unique text\n",
    "def createVocabList(dataSet):\n",
    "    vocab_set = []\n",
    "    for post in dataSet:\n",
    "        vocab_set += post\n",
    "    return list(set(vocab_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b1e817-4e30-4eac-a8ea-c61e3fb95b20",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['licks', 'take', 'mr', 'not', 'ate', 'maybe', 'I', 'quit', 'worthless', 'park', 'to', 'love', 'steak', 'dog', 'stop', 'posting', 'garbage', 'has', 'food', 'problems', 'buying', 'my', 'how', 'please', 'flea', 'help', 'stupid', 'is', 'so', 'him', 'dalmation', 'cute']\n"
     ]
    }
   ],
   "source": [
    "vocabList = createVocabList(postingList)\n",
    "print(vocabList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6d3397-2d43-4bc6-8ef6-b4024c63450c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transfer text into number and 1 = exist, 0 = not exist\n",
    "def setOfWords2Vec(vocabList, inputSet):\n",
    "    returnVec = [0]*len(vocabList)\n",
    "    for word in inputSet:\n",
    "        if word in vocabList:\n",
    "            returnVec[vocabList.index(word)] = 1\n",
    "    return returnVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc532a3-1a43-4f03-8049-5e5821906bc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(setOfWords2Vec(vocabList, postingList[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db342e08-bbc3-437c-bd1b-8c17687d72fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 1, 0, 0])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "postingList,listClasses = loadDataSet()\n",
    "vocabList = createVocabList(postingList)\n",
    "trainMat = []\n",
    "for doc in postingList:\n",
    "    trainMat.append(setOfWords2Vec(vocabList, doc))\n",
    "trainMat = np.array(trainMat)\n",
    "trainMat[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09f2830-212e-4232-82e6-7ade64c58ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Naive_bayes(X, y, y_value):\n",
    "    X = X + 1\n",
    "    prob_y = np.count_nonzero(y == y_value) / len(y)    # P(y)\n",
    "    class_index = np.where(y == y_value)\n",
    "    group = X[class_index]\n",
    "    prob_x = group.sum(axis=0) / group.sum()    # P(X|y)\n",
    "    return prob_x, prob_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc86f1a-3fd1-43a2-b1a7-3daf2bd5c2c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.02608696, 0.03478261, 0.02608696, 0.03478261, 0.02608696,\n",
       "        0.03478261, 0.02608696, 0.03478261, 0.04347826, 0.03478261,\n",
       "        0.03478261, 0.02608696, 0.02608696, 0.04347826, 0.03478261,\n",
       "        0.03478261, 0.03478261, 0.02608696, 0.03478261, 0.02608696,\n",
       "        0.03478261, 0.02608696, 0.02608696, 0.02608696, 0.02608696,\n",
       "        0.02608696, 0.05217391, 0.02608696, 0.02608696, 0.03478261,\n",
       "        0.02608696, 0.02608696]),\n",
       " 0.5)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for y_value in np.unique(listClasses):\n",
    "    prob_x, prob_y = Naive_bayes(trainMat, listClasses, y_value)\n",
    "prob_x, prob_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eaf2353",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
