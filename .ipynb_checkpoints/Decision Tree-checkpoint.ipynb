{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2403f6c-30ad-4968-b1c1-f06ed0e8ceec",
   "metadata": {},
   "source": [
    "# Decision Tree\n",
    "Link: https://www.analyticsvidhya.com/blog/2021/08/decision-tree-algorithm/\n",
    "\n",
    "## How to choose the root node\n",
    "For each categorical features, we identify the feature values (Y & N) into two leaves. For each leaves, we identify the target distribution\n",
    "\n",
    "## Prediction Model 1: Entropy\n",
    "Def: Entropy is an information theory metric that measures the impurity or uncertainty in a group of observations. \n",
    "- Bigger = impure\n",
    "- Smaller = pure\n",
    "- Range: 0-1\n",
    "\n",
    "Equation: $E= \\sum p(x)log(\\frac{1}{p(x)}) = -\\sum p(x)\\log_2(p(x))$\n",
    "\n",
    "## Prediction Model 2: Gini Impurity\n",
    "Def: tells us what is the probability of misclassifying an observation. Note that the lower the Gini the better the split and the lower the likelihood of misclassification. <br>\n",
    "Gini Impurity for a Leaf = 1 - [\\(probability of 'Yes'\\)$^2$ + \\(probability of 'No'\\)$^2$]<br>\n",
    "$Gini=1 - \\sum\\limits_{i-1}^{n}(p_i)^2$, for each leaves <br>\n",
    "Then, sum each Gini with its owned weight(number on its leaves / total number) $\\sum\\limits_{i=1}^{n}n_i/N*Gini_i$\n",
    "\n",
    "## Prediction Model: Information Gain\n",
    "Information gain as a measure of how much information a feature provides about a class. Information gain helps to determine the **order of attributes (the higher the first to be the node)** in the nodes of a decision tree. <br>\n",
    "E(Parent): the impurity of the y without split by features <br>\n",
    "E(Parent|Feature X1): the impurity of the y after split by features X1 <br>\n",
    "**Information Gain = E(Parent) - E(Parent|Feature X1)** <br>\n",
    "\n",
    "### Gini Impurity (Numeric Data)\n",
    "**Steps**\n",
    "1. Sort the feature values\n",
    "2. Calculate all the average value for all adjacent values. For example, [1, 3, 4, 6, 9, 13], average value: [2, 3.5, 5, 7.5, 11]\n",
    "3. For each average value, we use it for the classifier threshold.\n",
    "4. Calculate the Gini impurity(Categorical Data)\n",
    "5. Choose the threshold with the lowest Gini Impurity <br>\n",
    "\n",
    "After choosing the root node, each for internal nodes (branches), we can choose the next node based on the same Gini impurity method aboved until Gini impurity 0. But there may be **overfit**, so we can \n",
    "1. Pruning method\n",
    "2. the distribution of extreme enough, eg: 3:7 or 2:8 to stop the tree.\n",
    "3. stop the tree if people in the leave is small enough in a certain level "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db67b529-1648-4918-8466-deefa14d2276",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import math\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "np.set_printoptions(suppress=True)\n",
    "sns.set(rc={'figure.figsize':(10,8)})\n",
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df3d96b1-f098-4962-b0cf-2c6a27981542",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_iris(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d82e6aa-14fd-41e4-92d7-d0b68824e858",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Convert_num_cat(X, y):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff22a8a5-ce25-40b2-8f7a-3f6aeadaa783",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(X, y, threshold):\n",
    "    class1 = y[np.where(X[:, 0][X[:, 0]>6])]\n",
    "    class2 = y[np.where(X[:, 0][X[:, 0]<=6])]\n",
    "    return class1, class2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d1fbd75-3754-43fe-88d6-fcf7cdcefc32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5986259325205303"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groups = split(X, y, 2)\n",
    "entropy = 0\n",
    "total_num = np.sum([group.shape[0] for group in groups])\n",
    "for group in groups:\n",
    "    y = group\n",
    "    group_size = y.shape[0]\n",
    "    _, count = np.unique(y, return_counts=True)\n",
    "    p = count / group_size\n",
    "    weight = group_size / total_num\n",
    "    entropy += (p @ np.log(1/p)) * weight\n",
    "entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcec6bff-95b5-4381-8090-222880a41952",
   "metadata": {},
   "source": [
    "### Prediction model: Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fbff778e-8456-4caa-af60-d4bb1d8c793f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Entropy(*groups):\n",
    "    entropy = 0\n",
    "    total_num = np.sum([group.shape[0] for group in groups])\n",
    "    \n",
    "    for group in groups:\n",
    "        y = group[1]\n",
    "        group_size = y.shape[0]\n",
    "        _, count = np.unique(y, return_counts=True)\n",
    "        p = count / group_size\n",
    "        weight = group_size / total_num\n",
    "        \n",
    "        entropy += (p @ np.log(1/p)) * weight\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ecf718-1489-4eba-8d29-9d4fa47ff329",
   "metadata": {},
   "source": [
    "### Prediction model 2: Gini impurity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "814a2eef-1ec7-4881-9c86-3f4229d373bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Gini(*gorups):\n",
    "    gini = 0\n",
    "    total_num = np.sum([group.shape[0] for group in groups])\n",
    "    \n",
    "    for group in groups:\n",
    "        y = group[1]\n",
    "        group_size = y.shape[0]\n",
    "        _, count = np.unique(y, return_counts=True)\n",
    "        p = count / group_size\n",
    "        weight = group_size / total_num\n",
    "        \n",
    "        gini += (1 - np.sum(p**2)) * weight\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d7a90e-2f85-4a70-bd37-8013949bd0d5",
   "metadata": {},
   "source": [
    "### Information Gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "665f6b11-ec13-4bf6-8f58-d81cae040e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Information_Gain(X, y, feature_entropy):\n",
    "    target, counts = np.unique(y, return_counts=True)\n",
    "    p = counts / counts.sum()\n",
    "    parent_entropy = p @ np.log(1/p)\n",
    "    return parent_entropy - feature_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57aaf313-58d9-4c65-a7c5-51751ef973d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
