{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2289c00-c78f-4b01-a64e-0a43ef5687b6",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c273e808-a840-4837-9f2b-12f5b0bffc44",
   "metadata": {},
   "source": [
    "### Forward propagation\n",
    "$x_i$ = input layer<br>\n",
    "$w_i$ = weights<br>\n",
    "$b$ = bias<br>\n",
    "$z$ = hidden layer<br>\n",
    "$f(z)$ = activation<br>\n",
    "\n",
    "$z = \\sum\\limits_{i=1}^{n}x_iw_i + b$<br>\n",
    "Given Sigmoid as activation function: $f(H) = \\frac{1}{1+e^{-z}}$\n",
    "\n",
    "**Remark**<br>\n",
    "$w_i$ for b is 1<br>\n",
    "Which activation function to choose depends on the prediction output: number/category<br>\n",
    "If numerical output, MSE loss function will be used, reLU, linear(Identity) activation will be used.<br>\n",
    "If categorical output, cross entropy loss function will be used, sigmoid, tanh, softmax (non linear) activation function will be used.\n",
    "\n",
    "\n",
    "\n",
    "### Back propagation\n",
    "incrementally tweaking the networkâ€™s weights until the lowest possible cost value is obtained.\n",
    "\n",
    "### Partial derivative for $w_i$: $\\frac{\\partial C}{\\partial w_i} = \\frac{\\partial C}{\\partial \\hat{y}} * \\frac{\\partial \\hat{y}}{\\partial z} * \\frac{\\partial z}{\\partial w_i}$\n",
    "\n",
    "1. $\\frac{\\partial C}{\\partial \\hat{y}} = \\frac{\\partial}{\\partial \\hat{y}}\\frac{1}{n}\\sum\\limits_{i=1}^{n}(y_i-\\hat{y_i})^2 = \\frac{2}{n}\\sum\\limits_{1=1}^{n}(y_i-\\hat{y_i})$\n",
    "\n",
    "2. Given $\\sigma$ = Sigmoid function (different activation function has different derivative below)\n",
    "\n",
    "3. $\\frac{\\partial \\hat{y}}{\\partial z} = \\frac{\\partial}{\\partial z}\\sigma(z) = \\sigma(z) * (1-\\sigma(z)) $\n",
    "\n",
    "4. $\\frac{\\partial z}{\\partial w_i} = \\frac{\\partial}{\\partial w_i}\\sum\\limits_{i=1}^{n}x_iw_i+b = x_i$\n",
    "\n",
    "### $\\frac{\\partial C}{\\partial w_i} = \\frac{2}{n} * \\sum\\limits_{i=1}^{n}(y_i - \\hat{y_i}) * \\sigma(z) * (1-\\sigma(z)) * x_i$\n",
    "\n",
    "### Partial derivative for $b$\n",
    "### $\\frac{\\partial C}{\\partial b} = \\frac{2}{n} * \\sum\\limits_{i=1}^{n}(y_i-\\hat{y_i}) * \\sigma(z) * (1-\\sigma(z))$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721c2a1f-a631-4637-930e-f09d6dc7b484",
   "metadata": {},
   "source": [
    "# Cost Function\n",
    "##### MSE = $\\frac{1}{n}\\sum\\limits_{i=1}^{n}(y_i - \\hat{y_i})^2$\n",
    "\n",
    "##### Cross entropy = Sigmoid, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b597659c-49a8-45d6-a98e-907cddcb5f49",
   "metadata": {},
   "source": [
    "### learning algorithm\n",
    "1. Start with values (often random) for the network parameters (wij weights and bj biases).\n",
    "2. Take a set of examples of input data and pass them through the network to obtain their prediction.\n",
    "3. Compare these predictions obtained with the values of expected labels and calculate the loss with them.\n",
    "4. Perform the backpropagation in order to propagate this loss to each and every one of the parameters that make up the model of the neural network.\n",
    "5. Use this propagated information to update the parameters of the neural network with the gradient descent in a way that the total loss is reduced and a better model is obtained.\n",
    "6. Continue iterating in the previous steps until we consider that we have a good model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62207d73-5630-4240-a351-7fb5e1697f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822fedae-62a8-4593-93fa-605c2669cf81",
   "metadata": {},
   "source": [
    "### Activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802571c2-d8b6-4331-81e3-1c5043dba59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear(z):\n",
    "    pass z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51a6a40-a29c-4512-a78d-e759be4d391f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1+np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "112c0481-298d-410f-8905-3bea371a28bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(z):\n",
    "    pass np.tanh(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23c2e0a8-71f7-4dbe-a216-c9e32eba943a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(X):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "513a86b7-fbe3-4b15-a912-a95409d3efb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(z):\n",
    "    if z <= 0:\n",
    "        return 0\n",
    "    else:\n",
    "        z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0748c7-4fd9-4c50-9668-b356ada8c826",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
