{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad8e022c-95a2-430e-b2db-ca237285b0c2",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797f92e4-504c-4fb7-95ce-5391aa1063bb",
   "metadata": {},
   "source": [
    "### Model\n",
    "#### $P(y|X)=\\frac{P(X|y)P(y)}{P(X)}$\n",
    "\n",
    "if you have a lot of variables: <br>\n",
    "#### $P(y|x_1, ..., x_n)=\\frac{P(x_1|y)P(x_2|y)...P(x_n|y)P(y)}{P(x_1)P(x_2)...P(x_n)}$\n",
    "\n",
    "__*!!! since the denominator is the same for both probabilities, it can be omitted from the calculation, and only need to consider the numerator.*__\n",
    "###### $P(y|x_1, ..., x_n)=P(x_1|y)P(x_2|y)...P(x_n|y)P(y)$ \n",
    "<br>\n",
    "Naive Bayes is to use variable X to classify target y based on comparasion of probability of being target 1, target 2, target n\n",
    "\n",
    "### Assumption\n",
    "1. Features are independent to each other. \n",
    "2. Every feature is equally important.\n",
    "\n",
    "**Step**\n",
    "1. separate the dataset by target, you need to have a value count table for each class\n",
    "2. calculate the probability of each target: $P(Y)$ = number of Y / total number of entities\n",
    "3. for loop each target group:\n",
    "    * sum up the frequency for each unique word\n",
    "    * calculate the probability of each word in the target group, **remark: it is $P(X|y)$, conditional prob of x given by target y**\n",
    "    \n",
    "Now, we have $P(Y)$, $P(X|y)$ and P(X)<br>\n",
    "**Input new data** <br>\n",
    "for loop each target group:\n",
    "1. multiple the conditional probability for the input data: $P(y|x_1, ..., x_n)=\\frac{P(x_1|y)P(x_2|y)...P(x_n|y)P(y)}{P(x_1)P(x_2)...P(x_n)}$\n",
    "2. compare probability (likelihood) and assign the target with highest prob to the input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b1eac51-940b-453a-90a0-66301b705f2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd3c6e4b-9a60-46f8-a582-3d1928e416af",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'cp950' codec can't decode byte 0x92 in position 884: illegal multibyte sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m26\u001b[39m):\n\u001b[0;32m      4\u001b[0m     ham_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/email/ham/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.txt\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m     ham_file \u001b[38;5;241m=\u001b[39m \u001b[43mham_file\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m# ham_words = list(filter(None, re.split(r\"\\W+\", ham_file)))\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m#     ham_words = [j.lower() for j in ham_words if len(j) > 2]\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m#     ham_word_list.extend(ham_words)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m#     spam_words = [j.lower() for j in spam_words if len(j) > 2]\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m#     spam_word_list.extend(spam_words)\u001b[39;00m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'cp950' codec can't decode byte 0x92 in position 884: illegal multibyte sequence"
     ]
    }
   ],
   "source": [
    "ham_word_list = []\n",
    "spam_word_list = []\n",
    "for i in range(1, 26):\n",
    "    ham_file = open(f'data/email/ham/{i}.txt', 'r').read()\n",
    "    ham_words = list(filter(None, re.split(r\"\\W+\", ham_file)))\n",
    "    ham_words = [j.lower() for j in ham_words if len(j) > 2]\n",
    "    ham_word_list.extend(ham_words)\n",
    "    \n",
    "    spam_file = open(f'data/email/spam/{i}.txt', 'r').read()\n",
    "    spam_words = list(filter(None, re.split(r\"\\W+\", spam_file)))\n",
    "    spam_words = [j.lower() for j in spam_words if len(j) > 2]\n",
    "    spam_word_list.extend(spam_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "85d36a2a-c8b0-4b03-a290-e411dba8678a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ham_word_list = []\n",
    "spam_word_list = []\n",
    "ham_file = open(f'data/email/ham/full.txt', 'r', encoding='utf-8').read()\n",
    "ham_words = list(filter(None, re.split(r\"\\W+\", ham_file)))\n",
    "ham_words = [j.lower() for j in ham_words if len(j) > 2]\n",
    "ham_word_list.extend(ham_words)\n",
    "\n",
    "spam_file = open(f'data/email/spam/spam_full.txt', 'r', encoding='utf-8').read()\n",
    "spam_words = list(filter(None, re.split(r\"\\W+\", spam_file)))\n",
    "spam_words = [j.lower() for j in spam_words if len(j) > 2]\n",
    "spam_word_list.extend(spam_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "bf47b90a-31d4-4ce3-898a-cab6b4f509af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "value_count = Counter(ham_word_list)\n",
    "ham_df = pd.DataFrame.from_dict(value_count, orient='index', columns=['count']).reset_index()\n",
    "ham_df = ham_df.rename(columns={'index': 'word'})\n",
    "\n",
    "value_count = Counter(spam_word_list)\n",
    "spam_df = pd.DataFrame.from_dict(value_count, orient='index', columns=['count']).reset_index()\n",
    "spam_df = spam_df.rename(columns={'index': 'word'})\n",
    "\n",
    "ham_df['CP'] = ham_df['count'] / sum(ham_df['count'])\n",
    "spam_df['CP'] = spam_df['count'] / sum(spam_df['count'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e97cfaa-1094-4d2c-b182-bde2859ec601",
   "metadata": {},
   "source": [
    "# --------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "725b2741-eec7-400f-8e33-5f76e6155b90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def loadDataSet():\n",
    "    postingList=[['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],\n",
    "                 ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],\n",
    "                 ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],\n",
    "                 ['stop', 'posting', 'stupid', 'worthless', 'garbage'],\n",
    "                 ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],\n",
    "                 ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]\n",
    "    classVec = [0,1,0,1,0,1]    #1 is abusive, 0 not\n",
    "    return postingList,classVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a86e8131-eb8a-4740-953f-0d6a1003681e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create a unique word list\n",
    "def create_vocab_list(data_set):\n",
    "    vocab_set = set()  # create empty set\n",
    "    for document in data_set:\n",
    "        vocab_set = vocab_set | set(document)  # union of the two sets\n",
    "    return list(vocab_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "00cf751c-8ef5-48d2-bf11-4c303ae6d308",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# check if input word is in unique word list\n",
    "def set_of_words_2_vec(vocab_list, input_set):\n",
    "    return_vec = [0] * len(vocab_list)\n",
    "    for word in input_set:\n",
    "        if word in vocab_list:\n",
    "            return_vec[vocab_list.index(word)] = 1\n",
    "        else:\n",
    "            print(f\"The word '{word}' is not in my vocabulary!\")\n",
    "    return return_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "8eb5e2a7-98cd-4c0a-8598-99d9894be880",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listOPosts, listClasses = loadDataSet()\n",
    "myVocabList = create_vocab_list(listOPosts)\n",
    "set_of_words_2_vec(myVocabList, listOPosts[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9bb5a1-4e87-4077-be7a-f12506b5e99a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
