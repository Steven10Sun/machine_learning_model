{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad8e022c-95a2-430e-b2db-ca237285b0c2",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797f92e4-504c-4fb7-95ce-5391aa1063bb",
   "metadata": {},
   "source": [
    "### Model\n",
    "#### $P(y|X)=\\frac{P(X|y)P(y)}{P(X)}$\n",
    "\n",
    "if you have a lot of variables: <br>\n",
    "#### $P(y|x_1, ..., x_n)=\\frac{P(x_1|y)P(x_2|y)...P(x_n|y)P(y)}{P(x_1)P(x_2)...P(x_n)}$\n",
    "\n",
    "Naive Bayes is to use variable X to classify target y based on comparasion of probability of being target 1, target 2, target n\n",
    "\n",
    "**Step**\n",
    "1. separate the dataset by target\n",
    "2. calculate the probability of each target: $P(Y)$\n",
    "3. for loop each target group:\n",
    "    * sum up the frequency for each unique word\n",
    "    * calculate the probability of each word in the target group, **remark: it is $P(X|y)$, conditional prob of x given by target y**\n",
    "    \n",
    "Now, we have $P(Y)$, $P(X|y)$ and P(X)<br>\n",
    "**Input new data** <br>\n",
    "for loop each target group:\n",
    "1. multiple the conditional probability for the input data: $P(y|x_1, ..., x_n)=\\frac{P(x_1|y)P(x_2|y)...P(x_n|y)P(y)}{P(x_1)P(x_2)...P(x_n)}$\n",
    "2. compare probability (likelihood) and assign the target with highest prob to the input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "84fb9739-db08-4e9f-a8e2-97b9d9d4126c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0a1be16e-f627-49e1-9658-178156910fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadDataSet():\n",
    "    postingList=[['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],\n",
    "                 ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],\n",
    "                 ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],\n",
    "                 ['stop', 'posting', 'stupid', 'worthless', 'garbage'],\n",
    "                 ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],\n",
    "                 ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]\n",
    "    classVec = np.array([0,1,0,1,0,1])    #1 is abusive, 0 not\n",
    "    return postingList, classVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d26ab54a-acd1-479b-8e13-8af49f9a369e",
   "metadata": {},
   "outputs": [],
   "source": [
    "postingList,listClasses = loadDataSet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f9e6117b-0966-4805-9d5f-287f7a0b0cd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],\n",
       " ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],\n",
       " ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],\n",
       " ['stop', 'posting', 'stupid', 'worthless', 'garbage'],\n",
       " ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],\n",
       " ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "postingList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8584d3f8-71f8-4ec0-be68-5a1488f86942",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 1, 0, 1])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listClasses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bdc3979a-582b-4c98-bf98-67d05a41b4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract unique text\n",
    "def createVocabList(dataSet):\n",
    "    vocab_set = []\n",
    "    for post in dataSet:\n",
    "        vocab_set += post\n",
    "    return list(set(vocab_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "14b1e817-4e30-4eac-a8ea-c61e3fb95b20",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['licks', 'take', 'mr', 'not', 'ate', 'maybe', 'I', 'quit', 'worthless', 'park', 'to', 'love', 'steak', 'dog', 'stop', 'posting', 'garbage', 'has', 'food', 'problems', 'buying', 'my', 'how', 'please', 'flea', 'help', 'stupid', 'is', 'so', 'him', 'dalmation', 'cute']\n"
     ]
    }
   ],
   "source": [
    "vocabList = createVocabList(postingList)\n",
    "print(vocabList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9d6d3397-2d43-4bc6-8ef6-b4024c63450c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transfer text into number and 1 = exist, 0 = not exist\n",
    "def setOfWords2Vec(vocabList, inputSet):\n",
    "    returnVec = [0]*len(vocabList)\n",
    "    for word in inputSet:\n",
    "        if word in vocabList:\n",
    "            returnVec[vocabList.index(word)] = 1\n",
    "    return returnVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8cc532a3-1a43-4f03-8049-5e5821906bc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(setOfWords2Vec(vocabList, postingList[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "db342e08-bbc3-437c-bd1b-8c17687d72fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 1, 0, 0])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "postingList,listClasses = loadDataSet()\n",
    "vocabList = createVocabList(postingList)\n",
    "trainMat = []\n",
    "for doc in postingList:\n",
    "    trainMat.append(setOfWords2Vec(vocabList, doc))\n",
    "trainMat = np.array(trainMat)\n",
    "trainMat[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d09f2830-212e-4232-82e6-7ade64c58ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Naive_bayes(X, y, y_value):\n",
    "    X = X + 1\n",
    "    prob_y = np.count_nonzero(y == y_value) / len(y)    # P(y)\n",
    "    class_index = np.where(y == y_value)\n",
    "    group = X[class_index]\n",
    "    prob_x = group.sum(axis=0) / group.sum()    # P(X|y)\n",
    "    return prob_x, prob_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8cc86f1a-3fd1-43a2-b1a7-3daf2bd5c2c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.02608696, 0.03478261, 0.02608696, 0.03478261, 0.02608696,\n",
       "        0.03478261, 0.02608696, 0.03478261, 0.04347826, 0.03478261,\n",
       "        0.03478261, 0.02608696, 0.02608696, 0.04347826, 0.03478261,\n",
       "        0.03478261, 0.03478261, 0.02608696, 0.03478261, 0.02608696,\n",
       "        0.03478261, 0.02608696, 0.02608696, 0.02608696, 0.02608696,\n",
       "        0.02608696, 0.05217391, 0.02608696, 0.02608696, 0.03478261,\n",
       "        0.02608696, 0.02608696]),\n",
       " 0.5)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for y_value in np.unique(listClasses):\n",
    "    prob_x, prob_y = Naive_bayes(trainMat, listClasses, y_value)\n",
    "prob_x, prob_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b60402f-5210-4881-8675-58e3fccff575",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
