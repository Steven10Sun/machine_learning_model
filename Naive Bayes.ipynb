{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad8e022c-95a2-430e-b2db-ca237285b0c2",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797f92e4-504c-4fb7-95ce-5391aa1063bb",
   "metadata": {},
   "source": [
    "### Model\n",
    "#### $P(y|X)=\\frac{P(X|y)P(y)}{P(X)}$\n",
    "\n",
    "if you have a lot of variables: <br>\n",
    "#### $P(y|x_1, ..., x_n)=\\frac{P(x_1|y)P(x_2|y)...P(x_n|y)P(y)}{P(x_1)P(x_2)...P(x_n)}$\n",
    "\n",
    "__*!!! since the denominator is the same for both probabilities, it can be omitted from the calculation, and only need to consider the numerator.*__\n",
    "###### $P(y|x_1, ..., x_n)=P(x_1|y)P(x_2|y)...P(x_n|y)P(y)$ \n",
    "<br>\n",
    "Naive Bayes is to use variable X to classify target y based on comparasion of probability of being target 1, target 2, target n\n",
    "\n",
    "### Assumption\n",
    "1. Features are independent to each other. \n",
    "2. Every feature is equally important.\n",
    "\n",
    "**Step**\n",
    "1. separate the dataset by target, you need to have a value count table for each class\n",
    "2. calculate the probability of each target: $P(Y)$ = number of Y / total number of entities\n",
    "3. for loop each target group:\n",
    "    * sum up the frequency for each unique word\n",
    "    * calculate the probability of each word in the target group, **remark: it is $P(X|y)$, conditional prob of x given by target y**\n",
    "    \n",
    "Now, we have $P(Y)$, $P(X|y)$ and P(X)<br>\n",
    "**Input new data** <br>\n",
    "for loop each target group:\n",
    "1. multiple the conditional probability for the input data: $P(y|x_1, ..., x_n)=\\frac{P(x_1|y)P(x_2|y)...P(x_n|y)P(y)}{P(x_1)P(x_2)...P(x_n)}$\n",
    "2. compare probability (likelihood) and assign the target with highest prob to the input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "0b1eac51-940b-453a-90a0-66301b705f2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from collections import Counter\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "1cdfb800-95a9-4732-abab-1c7c0357c762",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadDataSet():\n",
    "    data = []\n",
    "    target = []\n",
    "    root_dir = \"data/email\"\n",
    "    for sub_folder in os.listdir(root_dir):\n",
    "        for txt_file in os.listdir(f'{root_dir}/{sub_folder}'):\n",
    "            with open(f'{root_dir}/{sub_folder}/{txt_file}', encoding=\"latin-1\") as f:\n",
    "                file_content = f.read()\n",
    "                words = list(filter(None, re.split(r\"\\W+\", file_content)))\n",
    "                words = [word.lower() for word in words if len(word) > 2]\n",
    "                data.append(words)\n",
    "                if sub_folder == 'ham':\n",
    "                    target.append(0) # ham = 0\n",
    "                else:\n",
    "                    target.append(1) # spam = \n",
    "    return data,target\n",
    "\n",
    "# create a unique word list from ham and spam\n",
    "def create_vocab_list(train_X):\n",
    "    vocab_set = set()  # create empty set\n",
    "    for document in train_X:\n",
    "        vocab_set = vocab_set | set(document)  # union of the two sets\n",
    "    return list(vocab_set)\n",
    "\n",
    "def Conditional_Prob(vocab_list, train_X, train_y):\n",
    "    return_vec_0 = np.ones(len(vocab_list)) # a value_count list for ham \n",
    "    return_vec_1 = np.ones(len(vocab_list)) # a value_count list for spam \n",
    "    for num in range(len(train_y)):\n",
    "        if train_y[num] == 0:\n",
    "            for word in train_X[num]:\n",
    "                return_vec_0[vocab_list.index(word)] += 1\n",
    "        if train_y[num] == 1: \n",
    "            for word in train_X[num]:\n",
    "                return_vec_1[vocab_list.index(word)] += 1\n",
    "    P_0 = Counter(train_y)[0] / len(train_y)\n",
    "    P_1 = 1 - P_0\n",
    "    CP_0 = return_vec_0 / np.sum(return_vec_0)\n",
    "    CP_1 = return_vec_1 / np.sum(return_vec_1)\n",
    "    return P_0, P_1, CP_0, CP_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "65ffef64-a4ad-4f90-9b15-9969f7872985",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Classify(vocab_list, P_0, P_1, CP_0, CP_1, test):\n",
    "    test_index = [vocab_list.index(i) for i in test if i in vocab_list]\n",
    "    p0 = np.sum(np.log(CP_0[test_index]))*P_0\n",
    "    p1 = np.sum(np.log(CP_1[test_index]))*P_1\n",
    "    if p0 > p1:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "93f00eb8-3e1a-46e1-9a96-44bb1fecb67c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n"
     ]
    }
   ],
   "source": [
    "data, target = loadDataSet()\n",
    "score_list = []\n",
    "\n",
    "# repeat 1000 to see how accurate the model\n",
    "for rep in range(1000):\n",
    "    # split data into train and test set\n",
    "    test_index = random.sample(range(50), 10)\n",
    "    train_X = [data[num] for num in range(len(data)) if num not in test_index]\n",
    "    train_y = [target[num] for num in range(len(target)) if num not in test_index]\n",
    "    test_X = [data[i] for i in test_index]\n",
    "    test_y = [target[i] for i in test_index]\n",
    "\n",
    "    vocab_list = create_vocab_list(train_X)\n",
    "    P_0, P_1, CP_0, CP_1 = Conditional_Prob(vocab_list, train_X, train_y)\n",
    "    result = []\n",
    "    for sample in test_X:\n",
    "        result.append(Classify(vocab_list, P_0, P_1, CP_0, CP_1, sample))\n",
    "    a = np.array(result)\n",
    "    b = np.array(test_y)\n",
    "    score = sum(a == b) / len(test_y)\n",
    "    score_list.append(score)\n",
    "    if rep % 100 == 0:\n",
    "        print(rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "86a399f4-f104-42fd-94e1-a82cf3771c11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.8940999999999941\n"
     ]
    }
   ],
   "source": [
    "print(f'Score: {sum(score_list)/1000}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
