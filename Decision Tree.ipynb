{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2403f6c-30ad-4968-b1c1-f06ed0e8ceec",
   "metadata": {},
   "source": [
    "# Decision Tree\n",
    "Link: https://www.analyticsvidhya.com/blog/2021/08/decision-tree-algorithm/\n",
    "\n",
    "## How to choose the root node\n",
    "For each categorical features, we identify the feature values (Y & N) into two leaves. For each leaves, we identify the target distribution\n",
    "\n",
    "## Prediction Model 1: Entropy\n",
    "Def: Entropy is an information theory metric that measures the impurity or uncertainty in a group of observations. \n",
    "- Bigger = impure\n",
    "- Smaller = pure\n",
    "- Range: 0-1\n",
    "\n",
    "Equation: $E= \\sum p(x)log(\\frac{1}{p(x)}) = -\\sum p(x)\\log_2(p(x))$\n",
    "\n",
    "## Prediction Model 2: Gini Impurity\n",
    "Def: tells us what is the probability of misclassifying an observation. Note that the lower the Gini the better the split and the lower the likelihood of misclassification. <br>\n",
    "Gini Impurity for a Leaf = 1 - [\\(probability of 'Yes'\\)$^2$ + \\(probability of 'No'\\)$^2$]<br>\n",
    "$Gini=1 - \\sum\\limits_{i-1}^{n}(p_i)^2$, for each leaves <br>\n",
    "Then, sum each Gini with its owned weight(number on its leaves / total number) $\\sum\\limits_{i=1}^{n}n_i/N*Gini_i$\n",
    "\n",
    "## Prediction Model: Information Gain\n",
    "Information gain as a measure of how much information a feature provides about a class. Information gain helps to determine the **order of attributes (the higher the first to be the node)** in the nodes of a decision tree. <br>\n",
    "E(Parent): the impurity of the y without split by features <br>\n",
    "E(Parent|Feature X1): the impurity of the y after split by features X1 <br>\n",
    "**Information Gain = E(Parent) - E(Parent|Feature X1)** <br>\n",
    "\n",
    "### Gini Impurity (Numeric Data)\n",
    "**Steps**\n",
    "1. Sort the feature values\n",
    "2. Calculate all the average value for all adjacent values. For example, [1, 3, 4, 6, 9, 13], average value: [2, 3.5, 5, 7.5, 11]\n",
    "3. For each average value, we use it for the classifier threshold.\n",
    "4. Calculate the Gini impurity(Categorical Data)\n",
    "5. Choose the threshold with the lowest Gini Impurity <br>\n",
    "\n",
    "After choosing the root node, each for internal nodes (branches), we can choose the next node based on the same Gini impurity method aboved until Gini impurity 0. But there may be **overfit**, so we can \n",
    "1. Pruning method\n",
    "2. the distribution of extreme enough, eg: 3:7 or 2:8 to stop the tree.\n",
    "3. stop the tree if people in the leave is small enough in a certain level "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "db67b529-1648-4918-8466-deefa14d2276",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import math\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "np.set_printoptions(suppress=True)\n",
    "sns.set(rc={'figure.figsize':(10,8)})\n",
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "df3d96b1-f098-4962-b0cf-2c6a27981542",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_iris(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d82e6aa-14fd-41e4-92d7-d0b68824e858",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Convert_num_cat(X, y):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff22a8a5-ce25-40b2-8f7a-3f6aeadaa783",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(X, y, threshold):\n",
    "    class1 = y[np.where(X[:, 0][X[:, 0]>6])]\n",
    "    class2 = y[np.where(X[:, 0][X[:, 0]<=6])]\n",
    "    return class1, class2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d1fbd75-3754-43fe-88d6-fcf7cdcefc32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5986259325205303"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groups = split(X, y, 2)\n",
    "entropy = 0\n",
    "total_num = np.sum([group.shape[0] for group in groups])\n",
    "for group in groups:\n",
    "    y = group\n",
    "    group_size = y.shape[0]\n",
    "    _, count = np.unique(y, return_counts=True)\n",
    "    p = count / group_size\n",
    "    weight = group_size / total_num\n",
    "    entropy += (p @ np.log(1/p)) * weight\n",
    "entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcec6bff-95b5-4381-8090-222880a41952",
   "metadata": {},
   "source": [
    "### Prediction model: Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fbff778e-8456-4caa-af60-d4bb1d8c793f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Entropy(*groups):\n",
    "    entropy = 0\n",
    "    total_num = np.sum([group.shape[0] for group in groups])\n",
    "    \n",
    "    for group in groups:\n",
    "        y = group[1]\n",
    "        group_size = y.shape[0]\n",
    "        _, count = np.unique(y, return_counts=True)\n",
    "        p = count / group_size\n",
    "        weight = group_size / total_num\n",
    "        \n",
    "        entropy += (p @ np.log(1/p)) * weight\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ecf718-1489-4eba-8d29-9d4fa47ff329",
   "metadata": {},
   "source": [
    "### Prediction model 2: Gini impurity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "814a2eef-1ec7-4881-9c86-3f4229d373bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Gini(*gorups):\n",
    "    gini = 0\n",
    "    total_num = np.sum([group.shape[0] for group in groups])\n",
    "    \n",
    "    for group in groups:\n",
    "        y = group[1]\n",
    "        group_size = y.shape[0]\n",
    "        _, count = np.unique(y, return_counts=True)\n",
    "        p = count / group_size\n",
    "        weight = group_size / total_num\n",
    "        \n",
    "        gini += (1 - np.sum(p**2)) * weight\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d7a90e-2f85-4a70-bd37-8013949bd0d5",
   "metadata": {},
   "source": [
    "### Information Gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "665f6b11-ec13-4bf6-8f58-d81cae040e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Information_Gain(X, y, feature_entropy):\n",
    "    target, counts = np.unique(y, return_counts=True)\n",
    "    p = counts / counts.sum()\n",
    "    parent_entropy = p @ np.log(1/p)\n",
    "    return parent_entropy - feature_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec092846-a267-4e62-8b30-b2e30ce5d184",
   "metadata": {},
   "source": [
    "### Prediction model: Entropy ----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "0eab29ff-3670-4796-8612-c97ef16d0727",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_iris(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "cb54507a-9632-41cd-b308-98e572fdd219",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['no lenses', 'soft', 'no lenses', 'hard', 'no lenses', 'soft',\n",
       "       'no lenses', 'hard', 'no lenses', 'soft', 'no lenses', 'hard',\n",
       "       'no lenses', 'soft', 'no lenses', 'no lenses', 'no lenses',\n",
       "       'no lenses', 'no lenses', 'hard', 'no lenses', 'soft', 'no lenses',\n",
       "       'no lenses'], dtype='<U10')"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fr=open('lense.txt', 'r')\n",
    "lenses = [inst.strip().split('\\t') for inst in fr.readlines()]\n",
    "lensesLabels = ['age', 'prescript', 'astigmatic', 'tearRate']\n",
    "X = np.array(lenses)[:, :-1]\n",
    "y = np.array(lenses)[:, -1]\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "2657ee80-8799-4d9c-92d9-c19d7bcc513f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Entropy(y):\n",
    "    _, count = np.unique(y, return_counts=True)\n",
    "    p = count / len(y)\n",
    "    entropy = p @ -np.log(p)\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "00791183-1e1a-474d-be2b-e89dbed034a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Split(X, y, col_index, value):\n",
    "    sub_group = X[X[:, col_index] == value]\n",
    "    # find the index of X and assign to y, return corresponding y\n",
    "    index = np.where(X[:, col_index] == value)[0]\n",
    "    return y[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "9f67e376-636b-4ad1-a518-1afd4ec4133e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0397207708399179"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Entropy(Split(X, y, 0, 'young'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5fba25b4-7770-41cf-8779-c5f7d6c563c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chooseBestFeatureToSplit(dataSet):\n",
    "    numFeatures = len(dataSet[0]) - 1      #the last column is used for the labels\n",
    "    baseEntropy = entropy(dataSet)\n",
    "    bestInfoGain = 0.0; bestFeature = -1\n",
    "    for i in range(numFeatures):        #iterate over all the features\n",
    "        featList = [example[i] for example in dataSet]#create a list of all the examples of this feature\n",
    "        uniqueVals = set(featList)       #get a set of unique values\n",
    "        newEntropy = 0.0\n",
    "        for value in uniqueVals:\n",
    "            subDataSet = splitDataSet(dataSet, i, value)\n",
    "            prob = len(subDataSet)/float(len(dataSet))\n",
    "            newEntropy += prob * entropy(subDataSet)     \n",
    "        infoGain = baseEntropy - newEntropy     #calculate the info gain; ie reduction in entropy\n",
    "        if (infoGain > bestInfoGain):       #compare this to the best gain so far\n",
    "            bestInfoGain = infoGain         #if better than current best, set to best\n",
    "            bestFeature = i\n",
    "    return bestFeature   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "90d2515b-ecb8-462f-a316-02dafb00e4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def majorityCnt(classList):\n",
    "    classCount={}\n",
    "    for vote in classList:\n",
    "        if vote not in classCount.keys(): \n",
    "            classCount[vote] = 0\n",
    "        classCount[vote] += 1\n",
    "    sortedClassCount = sorted(classCount.iteritems(), key=operator.itemgetter(1), reverse=True)\n",
    "    return sortedClassCount[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "36076cf5-bfcb-4c0e-a5b5-57880bb2ca63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['no surfacing', 'flippers'] 0 no surfacing\n",
      "{'no surfacing': {}}\n",
      "[1, 1, 1, 0, 0]\n",
      "{0, 1}\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'createTree' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [57], line 19\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m value \u001b[38;5;129;01min\u001b[39;00m uniqueVals:\n\u001b[0;32m     18\u001b[0m     subLabels \u001b[38;5;241m=\u001b[39m labels[:]\n\u001b[1;32m---> 19\u001b[0m     myTree[bestFeatLabel][value] \u001b[38;5;241m=\u001b[39m \u001b[43mcreateTree\u001b[49m(splitDataSet(dataSet, bestFeat, value),subLabels)\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28mprint\u001b[39m(myTree)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'createTree' is not defined"
     ]
    }
   ],
   "source": [
    "def createTree(dataSet,labels):\n",
    "    dataSet, labels = createDataSet()\n",
    "    classList = [example[-1] for example in dataSet]\n",
    "    if classList.count(classList[0]) == len(classList):\n",
    "        print(classList[0])\n",
    "    if len(dataSet[0]) == 1:\n",
    "        print(majorityCnt(classList))\n",
    "    bestFeat = chooseBestFeatureToSplit(dataSet)\n",
    "    bestFeatLabel = labels[bestFeat]\n",
    "    print(labels, bestFeat, bestFeatLabel)\n",
    "    myTree = {bestFeatLabel:{}}\n",
    "    print(myTree)\n",
    "    del(labels[bestFeat])\n",
    "    featValues = [example[bestFeat] for example in dataSet]\n",
    "    print(featValues)\n",
    "    uniqueVals = set(featValues)\n",
    "    print(uniqueVals)\n",
    "    for value in uniqueVals:\n",
    "        subLabels = labels[:]\n",
    "        myTree[bestFeatLabel][value] = createTree(splitDataSet(dataSet, bestFeat, value),subLabels)\n",
    "        print(myTree)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
